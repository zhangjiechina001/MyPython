{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]\n",
      " [ 0.59282297 -2.1229296  -0.72289723]]\n",
      "After 0 training step,cross entropy on all data is 1.56615\n",
      "After 1000 training step,cross entropy on all data is 0.608266\n",
      "After 2000 training step,cross entropy on all data is 0.608132\n",
      "After 3000 training step,cross entropy on all data is 0.608198\n",
      "After 4000 training step,cross entropy on all data is 0.608113\n",
      "After 5000 training step,cross entropy on all data is 0.608146\n",
      "After 6000 training step,cross entropy on all data is 0.608103\n",
      "After 7000 training step,cross entropy on all data is 0.608115\n",
      "After 8000 training step,cross entropy on all data is 0.608099\n",
      "After 9000 training step,cross entropy on all data is 0.608105\n",
      "After 10000 training step,cross entropy on all data is 0.608099\n",
      "After 11000 training step,cross entropy on all data is 0.608103\n",
      "After 12000 training step,cross entropy on all data is 0.608098\n",
      "After 13000 training step,cross entropy on all data is 0.608101\n",
      "After 14000 training step,cross entropy on all data is 0.608097\n",
      "After 15000 training step,cross entropy on all data is 0.608135\n",
      "After 16000 training step,cross entropy on all data is 0.608096\n",
      "After 17000 training step,cross entropy on all data is 0.6081\n",
      "After 18000 training step,cross entropy on all data is 0.608096\n",
      "After 19000 training step,cross entropy on all data is 0.608092\n",
      "After 20000 training step,cross entropy on all data is 0.608096\n",
      "After 21000 training step,cross entropy on all data is 0.608101\n",
      "After 22000 training step,cross entropy on all data is 0.608096\n",
      "After 23000 training step,cross entropy on all data is 0.6081\n",
      "After 24000 training step,cross entropy on all data is 0.608096\n",
      "After 25000 training step,cross entropy on all data is 0.608099\n",
      "After 26000 training step,cross entropy on all data is 0.608096\n",
      "After 27000 training step,cross entropy on all data is 0.608098\n",
      "After 28000 training step,cross entropy on all data is 0.608096\n",
      "After 29000 training step,cross entropy on all data is 0.608096\n",
      "After 30000 training step,cross entropy on all data is 0.608096\n",
      "After 31000 training step,cross entropy on all data is 0.608097\n",
      "After 32000 training step,cross entropy on all data is 0.608095\n",
      "After 33000 training step,cross entropy on all data is 0.608171\n",
      "After 34000 training step,cross entropy on all data is 0.608095\n",
      "After 35000 training step,cross entropy on all data is 0.608096\n",
      "After 36000 training step,cross entropy on all data is 0.608095\n",
      "After 37000 training step,cross entropy on all data is 0.608096\n",
      "After 38000 training step,cross entropy on all data is 0.608095\n",
      "After 39000 training step,cross entropy on all data is 0.608101\n",
      "After 40000 training step,cross entropy on all data is 0.608095\n",
      "After 41000 training step,cross entropy on all data is 0.608096\n",
      "After 42000 training step,cross entropy on all data is 0.608095\n",
      "After 43000 training step,cross entropy on all data is 0.608096\n",
      "After 44000 training step,cross entropy on all data is 0.608097\n",
      "After 45000 training step,cross entropy on all data is 0.608096\n",
      "After 46000 training step,cross entropy on all data is 0.608096\n",
      "After 47000 training step,cross entropy on all data is 0.608098\n",
      "After 48000 training step,cross entropy on all data is 0.608097\n",
      "After 49000 training step,cross entropy on all data is 0.608097\n",
      "[[-1.195153    0.00280252  0.8861235 ]\n",
      " [-1.0438643   0.00206035 -0.56703293]]\n",
      "[[-1.1558839e-04  7.7863455e-01  5.5586919e-04]\n",
      " [ 2.6860037e-01 -5.0300127e-04  3.9136823e-02]\n",
      " [-4.0868836e-05 -1.3020791e+00 -4.4674417e-03]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "#定义训练数据batch的大小\n",
    "batch_size=8\n",
    "\n",
    "#定义神经网络的参数，这里还是沿用3\n",
    "w1=tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2=tf.Variable(tf.random_normal([3,3],stddev=1,seed=1))\n",
    "w3=tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "\n",
    "#placeholder是TensorFlow的占位符节点，shape选填\n",
    "x=tf.placeholder(tf.float32,shape=(None,2),name='x-input')\n",
    "y_=tf.placeholder(tf.float32,shape=(None,1),name='y-input')\n",
    "\n",
    "a=tf.matmul(x,w1)\n",
    "b=tf.matmul(a,w2)\n",
    "y=tf.matmul(b,w3)\n",
    "\n",
    "y=tf.sigmoid(y)\n",
    "#熵越大，说明不确定性越大\n",
    "cross_entropy= -tf.reduce_mean(\n",
    "    y_*tf.log(tf.clip_by_value(y,-1,1.0))+\n",
    "    (1 - y_)*tf.log(tf.clip_by_value(1-y,-1,1.0)))\n",
    "#tf.clip_by_value(A, min, max)：输入一个张量A，把A中的每一个元素的值都压缩在min和max之间。\n",
    "#小于min的让它等于min，大于max的元素的值等于max\n",
    "train_step=tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "#通过随机数生成一个模拟数据集\n",
    "rdm=RandomState(1)\n",
    "dataset_size=128\n",
    "X=rdm.rand(dataset_size,2)\n",
    "'''定义规则出来的样本标签。在这里所有x1+x2<1的样例被认为是正样本'''\n",
    "Y=[[int(x1+x2<1)] for (x1,x2) in X]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #初始化变量\n",
    "    init_op=tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    STEPS=50000\n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        #每次选取batch_size个样本进行训练\n",
    "        start=(i*batch_size)%dataset_size#从128个样本中选取\n",
    "        end=min(start+batch_size,dataset_size)#规模为最大为8的数据集\n",
    "\n",
    "        #通过选取的样本训练神经网络并更新参数\n",
    "        sess.run(train_step,\n",
    "                 feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "    \n",
    "        if(i%1000==0):\n",
    "            #每隔一段时间计算在所有数据上的交叉熵并输出\n",
    "            total_cross_entropy=sess.run(cross_entropy,feed_dict={x:X,y_:Y})\n",
    "            print(\"After %d training step,cross entropy on all data is %g\"%(i,total_cross_entropy))\n",
    "\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
